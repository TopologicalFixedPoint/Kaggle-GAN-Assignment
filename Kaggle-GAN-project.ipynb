{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":21755,"databundleVersionId":1475600,"sourceType":"competition"}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import keras\nfrom keras import ops\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport tensorflow as tf\nfrom PIL import Image","metadata":{"ExecuteTime":{"end_time":"2024-12-12T21:11:45.486591Z","start_time":"2024-12-12T21:11:45.483526Z"},"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T21:32:25.234142Z","iopub.execute_input":"2024-12-12T21:32:25.234891Z","iopub.status.idle":"2024-12-12T21:32:25.239423Z","shell.execute_reply.started":"2024-12-12T21:32:25.234853Z","shell.execute_reply":"2024-12-12T21:32:25.238242Z"}},"outputs":[],"execution_count":null},{"cell_type":"raw","source":"The Kaggle Competition that this notebook is building on is found at https://www.kaggle.com/competitions/gan-getting-started/overview. The problem statement is given photos of various environments to look like Monet stle paintings. The data used in this notebook can be found at https://www.kaggle.com/competitions/gan-getting-started/data.","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"#The first step is to load the data. The data is separated between the monet paintings that will be taken as real images for the discriminator\n#The images are resized to 64x64 to ensure the model trains faster and would require less resources\nmonet_data = tf.keras.utils.image_dataset_from_directory(\"/kaggle/input/gan-getting-started/monet_jpg\", seed=42, image_size=(64, 64), label_mode=None)","metadata":{"collapsed":false,"ExecuteTime":{"end_time":"2024-12-12T18:25:41.460932Z","start_time":"2024-12-12T18:25:41.093667Z"},"jupyter":{"outputs_hidden":false},"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T21:32:29.309359Z","iopub.execute_input":"2024-12-12T21:32:29.310126Z","iopub.status.idle":"2024-12-12T21:32:32.319351Z","shell.execute_reply.started":"2024-12-12T21:32:29.310091Z","shell.execute_reply":"2024-12-12T21:32:32.318525Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Next the data is normalized. There are 300 Monet paintings and 7038 photos in the Kaggle dataset\nmonet_norm_data = monet_data.map(lambda x: x / 255.0)","metadata":{"collapsed":false,"ExecuteTime":{"end_time":"2024-12-12T18:25:42.547915Z","start_time":"2024-12-12T18:25:42.536814Z"},"jupyter":{"outputs_hidden":false},"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T21:32:32.320808Z","iopub.execute_input":"2024-12-12T21:32:32.321088Z","iopub.status.idle":"2024-12-12T21:32:32.344572Z","shell.execute_reply.started":"2024-12-12T21:32:32.321060Z","shell.execute_reply":"2024-12-12T21:32:32.343636Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Here I plot some of the Monet paintings to get a sense of what the goal for the GAN is\nplt.imshow(list(monet_norm_data.as_numpy_iterator())[5][5])","metadata":{"collapsed":false,"ExecuteTime":{"end_time":"2024-12-12T18:25:43.989240Z","start_time":"2024-12-12T18:25:43.869628Z"},"jupyter":{"outputs_hidden":false},"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T21:32:32.345797Z","iopub.execute_input":"2024-12-12T21:32:32.346164Z","iopub.status.idle":"2024-12-12T21:32:33.096906Z","shell.execute_reply.started":"2024-12-12T21:32:32.346121Z","shell.execute_reply":"2024-12-12T21:32:33.095950Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.imshow(list(monet_norm_data.as_numpy_iterator())[8][8])","metadata":{"collapsed":false,"ExecuteTime":{"end_time":"2024-12-12T18:25:45.634421Z","start_time":"2024-12-12T18:25:45.526083Z"},"jupyter":{"outputs_hidden":false},"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T21:32:33.098413Z","iopub.execute_input":"2024-12-12T21:32:33.098724Z","iopub.status.idle":"2024-12-12T21:32:33.394090Z","shell.execute_reply.started":"2024-12-12T21:32:33.098694Z","shell.execute_reply":"2024-12-12T21:32:33.393003Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"monet_norm_data.element_spec","metadata":{"collapsed":false,"ExecuteTime":{"end_time":"2024-12-12T18:25:56.651580Z","start_time":"2024-12-12T18:25:56.647707Z"},"jupyter":{"outputs_hidden":false},"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T21:32:33.864892Z","iopub.execute_input":"2024-12-12T21:32:33.865571Z","iopub.status.idle":"2024-12-12T21:32:33.871224Z","shell.execute_reply.started":"2024-12-12T21:32:33.865506Z","shell.execute_reply":"2024-12-12T21:32:33.870277Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Each photo in the dataset is 64x64 and then is divided into 3 color channels. With the data normalized, the next step is to create the model.","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"#For the model I will utilize DCGANs with different parameters\n#I started with the base model shown in the keras guide to serve as a control\nbase_generator = keras.Sequential([\n    keras.layers.InputLayer(shape=(128,)),\n    keras.layers.Dense(8*8*128),\n    keras.layers.Reshape((8,8,128)),\n    keras.layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding=\"same\"),\n    keras.layers.LeakyReLU(negative_slope=0.2),\n    keras.layers.Conv2DTranspose(256, kernel_size=4, strides=2, padding=\"same\"),\n    keras.layers.LeakyReLU(negative_slope=0.2),\n    keras.layers.Conv2DTranspose(512, kernel_size=4, strides=2, padding=\"same\"),\n    keras.layers.LeakyReLU(negative_slope=0.2),\n    keras.layers.Conv2D(3, kernel_size=5, padding=\"same\", activation=\"sigmoid\"),\n], name=\"control_generator\")\n\n","metadata":{"collapsed":false,"ExecuteTime":{"end_time":"2024-12-12T18:30:50.551079Z","start_time":"2024-12-12T18:30:50.525442Z"},"jupyter":{"outputs_hidden":false},"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T21:32:35.560573Z","iopub.execute_input":"2024-12-12T21:32:35.560911Z","iopub.status.idle":"2024-12-12T21:32:35.636298Z","shell.execute_reply.started":"2024-12-12T21:32:35.560881Z","shell.execute_reply":"2024-12-12T21:32:35.635333Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#The first change is to make the kernel smaller\nkernel_generator = keras.Sequential([\n    keras.layers.InputLayer(shape=(128,)),\n    keras.layers.Dense(8*8*128),\n    keras.layers.Reshape((8,8,128)),\n    keras.layers.Conv2DTranspose(128, kernel_size=2, strides=2, padding=\"same\"),\n    keras.layers.LeakyReLU(negative_slope=0.2),\n    keras.layers.Conv2DTranspose(256, kernel_size=2, strides=2, padding=\"same\"),\n    keras.layers.LeakyReLU(negative_slope=0.2),\n    keras.layers.Conv2DTranspose(512, kernel_size=2, strides=2, padding=\"same\"),\n    keras.layers.LeakyReLU(negative_slope=0.2),\n    keras.layers.Conv2D(3, kernel_size=5, padding=\"same\", activation=\"sigmoid\"),\n], name=\"kernel_generator\")\nkernel_generator.summary()","metadata":{"collapsed":false,"ExecuteTime":{"end_time":"2024-12-12T18:48:13.634699Z","start_time":"2024-12-12T18:48:13.602345Z"},"jupyter":{"outputs_hidden":false},"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T21:32:36.307875Z","iopub.execute_input":"2024-12-12T21:32:36.308223Z","iopub.status.idle":"2024-12-12T21:32:36.374282Z","shell.execute_reply.started":"2024-12-12T21:32:36.308192Z","shell.execute_reply":"2024-12-12T21:32:36.373388Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#The next modification is to make the slope of the leakyrelu steeper\nrelu_generator = keras.Sequential([\n    keras.layers.InputLayer(shape=(128,)),\n    keras.layers.Dense(8*8*128),\n    keras.layers.Reshape((8,8,128)),\n    keras.layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding=\"same\"),\n    keras.layers.LeakyReLU(negative_slope=0.5),\n    keras.layers.Conv2DTranspose(256, kernel_size=4, strides=2, padding=\"same\"),\n    keras.layers.LeakyReLU(negative_slope=0.5),\n    keras.layers.Conv2DTranspose(512, kernel_size=4, strides=2, padding=\"same\"),\n    keras.layers.LeakyReLU(negative_slope=0.5),\n    keras.layers.Conv2D(3, kernel_size=5, padding=\"same\", activation=\"sigmoid\"),\n], name=\"relu_generator\")\nrelu_generator.summary()","metadata":{"collapsed":false,"ExecuteTime":{"end_time":"2024-12-12T18:47:59.634706Z","start_time":"2024-12-12T18:47:59.598332Z"},"jupyter":{"outputs_hidden":false},"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T21:32:37.145432Z","iopub.execute_input":"2024-12-12T21:32:37.145814Z","iopub.status.idle":"2024-12-12T21:32:37.204936Z","shell.execute_reply.started":"2024-12-12T21:32:37.145780Z","shell.execute_reply":"2024-12-12T21:32:37.204183Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Similarly for the discriminator I started with the example as the control\nbase_discriminator = keras.Sequential([\n    keras.Input(shape=(64, 64, 3)),\n    keras.layers.Conv2D(64, kernel_size=4, strides=2, padding=\"same\"),\n    keras.layers.LeakyReLU(negative_slope=0.2),\n    keras.layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n    keras.layers.LeakyReLU(negative_slope=0.2),\n    keras.layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n    keras.layers.LeakyReLU(negative_slope=0.2),\n    keras.layers.Flatten(),\n    keras.layers.Dropout(0.2),\n    keras.layers.Dense(1, activation=\"sigmoid\"),\n], name=\"base_discriminator\")\nbase_discriminator.summary()","metadata":{"collapsed":false,"ExecuteTime":{"end_time":"2024-12-12T18:50:38.461859Z","start_time":"2024-12-12T18:50:38.431254Z"},"jupyter":{"outputs_hidden":false},"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T21:32:37.874408Z","iopub.execute_input":"2024-12-12T21:32:37.875295Z","iopub.status.idle":"2024-12-12T21:32:37.949130Z","shell.execute_reply.started":"2024-12-12T21:32:37.875257Z","shell.execute_reply":"2024-12-12T21:32:37.947985Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#The only change I tested with the discriminator is to increase the slope of the leaky relu function\nslope_discriminator = keras.Sequential([\n    keras.Input(shape=(64, 64, 3)),\n    keras.layers.Conv2D(64, kernel_size=4, strides=2, padding=\"same\"),\n    keras.layers.LeakyReLU(negative_slope=0.5),\n    keras.layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n    keras.layers.LeakyReLU(negative_slope=0.5),\n    keras.layers.Conv2D(128, kernel_size=4, strides=2, padding=\"same\"),\n    keras.layers.LeakyReLU(negative_slope=0.5),\n    keras.layers.Flatten(),\n    keras.layers.Dropout(0.2),\n    keras.layers.Dense(1, activation=\"sigmoid\"),\n], name=\"slope_discriminator\")\nslope_discriminator.summary()","metadata":{"collapsed":false,"ExecuteTime":{"end_time":"2024-12-12T18:54:08.697982Z","start_time":"2024-12-12T18:54:08.670012Z"},"jupyter":{"outputs_hidden":false},"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T21:32:38.617483Z","iopub.execute_input":"2024-12-12T21:32:38.618256Z","iopub.status.idle":"2024-12-12T21:32:38.675487Z","shell.execute_reply.started":"2024-12-12T21:32:38.618218Z","shell.execute_reply":"2024-12-12T21:32:38.674580Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#The followimg code to assemble the GAN and override the training state can be found at https://keras.io/examples/generative/dcgan_overriding_train_step/#override-trainstep\nclass GAN(keras.Model):\n    def __init__(self, discriminator, generator):\n        super().__init__()\n        self.discriminator = discriminator\n        self.generator = generator\n\n    def compile(self, d_optimizer, g_optimizer, loss_fn):\n        super().compile()\n        self.d_optimizer = d_optimizer\n        self.g_optimizer = g_optimizer\n        self.loss_fn = loss_fn\n        self.d_loss_metric = keras.metrics.Mean(name=\"d_loss\")\n        self.g_loss_metric = keras.metrics.Mean(name=\"g_loss\")\n\n    @property\n    def metrics(self):\n        return [self.d_loss_metric, self.g_loss_metric]\n\n    def train_step(self, real_images):\n        # Sample random points in the latent space\n        batch_size = ops.shape(real_images)[0]\n        random_latent_vectors = keras.random.normal(\n            shape=(batch_size, 128), seed=42\n        )\n\n        # Decode them to fake images\n        generated_images = self.generator(random_latent_vectors)\n\n        # Combine them with real images\n        combined_images = ops.concatenate([generated_images, real_images], axis=0)\n\n        # Assemble labels discriminating real from fake images\n        labels = ops.concatenate(\n            [ops.ones((batch_size, 1)), ops.zeros((batch_size, 1))], axis=0\n        )\n        # Add random noise to the labels - important trick!\n        labels += 0.05 * tf.random.uniform(tf.shape(labels))\n\n        # Train the discriminator\n        with tf.GradientTape() as tape:\n            predictions = self.discriminator(combined_images)\n            d_loss = self.loss_fn(labels, predictions)\n        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n        self.d_optimizer.apply_gradients(\n            zip(grads, self.discriminator.trainable_weights)\n        )\n\n        # Sample random points in the latent space\n        random_latent_vectors = keras.random.normal(\n            shape=(batch_size, 128), seed=42\n        )\n\n        # Assemble labels that say \"all real images\"\n        misleading_labels = ops.zeros((batch_size, 1))\n\n        # Train the generator (note that we should *not* update the weights\n        # of the discriminator)!\n        with tf.GradientTape() as tape:\n            predictions = self.discriminator(self.generator(random_latent_vectors))\n            g_loss = self.loss_fn(misleading_labels, predictions)\n        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n\n        # Update metrics\n        self.d_loss_metric.update_state(d_loss)\n        self.g_loss_metric.update_state(g_loss)\n        return {\n            \"d_loss\": self.d_loss_metric.result(),\n            \"g_loss\": self.g_loss_metric.result(),\n        }","metadata":{"collapsed":false,"ExecuteTime":{"end_time":"2024-12-12T19:07:05.548201Z","start_time":"2024-12-12T19:07:05.540781Z"},"jupyter":{"outputs_hidden":false},"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T21:32:39.651652Z","iopub.execute_input":"2024-12-12T21:32:39.652340Z","iopub.status.idle":"2024-12-12T21:32:39.662878Z","shell.execute_reply.started":"2024-12-12T21:32:39.652300Z","shell.execute_reply":"2024-12-12T21:32:39.662038Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#In total as there are 2 discriminators and 3 generators, there are 6 possible GANs that can be formed\nbase_gan = GAN(base_discriminator, base_generator)\ng_slope_gan = GAN(base_discriminator, relu_generator)\nkernel_gan = GAN(base_discriminator, kernel_generator)\nd_slope_gan = GAN(slope_discriminator, base_generator)\nslope_gan = GAN(slope_discriminator, relu_generator)\nleaky_kernel_gan = GAN(slope_discriminator, kernel_generator)\n","metadata":{"collapsed":false,"ExecuteTime":{"end_time":"2024-12-12T19:17:55.048750Z","start_time":"2024-12-12T19:17:55.043252Z"},"jupyter":{"outputs_hidden":false},"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T21:32:40.643372Z","iopub.execute_input":"2024-12-12T21:32:40.643744Z","iopub.status.idle":"2024-12-12T21:32:40.653445Z","shell.execute_reply.started":"2024-12-12T21:32:40.643710Z","shell.execute_reply":"2024-12-12T21:32:40.652724Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#As per the paper https://arxiv.org/pdf/1511.06434 the researchers found that the optimal learning rate is 0.0002\nbase_gan.compile(keras.optimizers.Adam(learning_rate=0.0002), keras.optimizers.Adam(learning_rate=0.0002), keras.losses.BinaryCrossentropy())\nhistory = base_gan.fit(monet_norm_data, epochs=100)","metadata":{"collapsed":false,"ExecuteTime":{"end_time":"2024-12-12T19:41:32.999393Z","start_time":"2024-12-12T19:30:31.827564Z"},"jupyter":{"outputs_hidden":false},"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T21:32:43.352681Z","iopub.execute_input":"2024-12-12T21:32:43.353041Z","iopub.status.idle":"2024-12-12T21:37:41.822511Z","shell.execute_reply.started":"2024-12-12T21:32:43.353008Z","shell.execute_reply":"2024-12-12T21:37:41.821599Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Generally the loss of the discriminator stayed low as the generator took ~25 epochs to have a consistently low loss then it spikes a bit around 80 epochs\nplt.plot(history.history[\"d_loss\"])\nplt.plot(history.history[\"g_loss\"])","metadata":{"collapsed":false,"ExecuteTime":{"end_time":"2024-12-12T19:43:33.612744Z","start_time":"2024-12-12T19:43:33.565005Z"},"jupyter":{"outputs_hidden":false},"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T21:37:41.824036Z","iopub.execute_input":"2024-12-12T21:37:41.824311Z","iopub.status.idle":"2024-12-12T21:37:41.985442Z","shell.execute_reply.started":"2024-12-12T21:37:41.824283Z","shell.execute_reply":"2024-12-12T21:37:41.984468Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Overall each model will be trained over 50 epochs\ng_slope_gan.compile(keras.optimizers.Adam(learning_rate=0.0002), keras.optimizers.Adam(learning_rate=0.0002), keras.losses.BinaryCrossentropy())\nhistory = g_slope_gan.fit(monet_norm_data, epochs=100)","metadata":{"collapsed":false,"ExecuteTime":{"end_time":"2024-12-12T19:54:20.287014Z","start_time":"2024-12-12T19:46:20.797999Z"},"jupyter":{"outputs_hidden":false},"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T21:38:35.225321Z","iopub.execute_input":"2024-12-12T21:38:35.225700Z","iopub.status.idle":"2024-12-12T21:43:24.430423Z","shell.execute_reply.started":"2024-12-12T21:38:35.225665Z","shell.execute_reply":"2024-12-12T21:43:24.429388Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Generally the loss of the generator diverges for the first 30 epochs but then converges and performs better than the baseline GAN\nplt.plot(history.history[\"d_loss\"])\nplt.plot(history.history[\"g_loss\"])","metadata":{"collapsed":false,"ExecuteTime":{"end_time":"2024-12-12T19:54:20.361174Z","start_time":"2024-12-12T19:54:20.288024Z"},"jupyter":{"outputs_hidden":false},"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T21:43:24.432603Z","iopub.execute_input":"2024-12-12T21:43:24.433029Z","iopub.status.idle":"2024-12-12T21:43:24.682267Z","shell.execute_reply.started":"2024-12-12T21:43:24.432982Z","shell.execute_reply":"2024-12-12T21:43:24.681415Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"kernel_gan.compile(keras.optimizers.Adam(learning_rate=0.0002), keras.optimizers.Adam(learning_rate=0.0002), keras.losses.BinaryCrossentropy())\nhistory = kernel_gan.fit(monet_norm_data, epochs=100)","metadata":{"collapsed":false,"ExecuteTime":{"end_time":"2024-12-12T20:00:23.457066Z","start_time":"2024-12-12T19:56:43.723019Z"},"jupyter":{"outputs_hidden":false},"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T21:44:06.196985Z","iopub.execute_input":"2024-12-12T21:44:06.197700Z","iopub.status.idle":"2024-12-12T21:46:52.458565Z","shell.execute_reply.started":"2024-12-12T21:44:06.197659Z","shell.execute_reply":"2024-12-12T21:46:52.457768Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Both models seemed to stay fairly low, it is unclear how well the generator learned\nplt.plot(history.history[\"d_loss\"])\nplt.plot(history.history[\"g_loss\"])","metadata":{"collapsed":false,"ExecuteTime":{"end_time":"2024-12-12T20:00:23.524600Z","start_time":"2024-12-12T20:00:23.458109Z"},"jupyter":{"outputs_hidden":false},"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T21:46:52.460071Z","iopub.execute_input":"2024-12-12T21:46:52.460382Z","iopub.status.idle":"2024-12-12T21:46:52.704957Z","shell.execute_reply.started":"2024-12-12T21:46:52.460351Z","shell.execute_reply":"2024-12-12T21:46:52.704022Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"d_slope_gan.compile(keras.optimizers.Adam(learning_rate=0.0002), keras.optimizers.Adam(learning_rate=0.0002), keras.losses.BinaryCrossentropy())\nhistory = d_slope_gan.fit(monet_norm_data, epochs=100)","metadata":{"collapsed":false,"ExecuteTime":{"end_time":"2024-12-12T20:20:37.462063Z","start_time":"2024-12-12T20:04:20.569684Z"},"jupyter":{"outputs_hidden":false},"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T21:48:00.317441Z","iopub.execute_input":"2024-12-12T21:48:00.318303Z","iopub.status.idle":"2024-12-12T21:52:44.679504Z","shell.execute_reply.started":"2024-12-12T21:48:00.318265Z","shell.execute_reply":"2024-12-12T21:52:44.678782Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#From the plot the generator had trouble fooling the discriminator\nplt.plot(history.history[\"d_loss\"])\nplt.plot(history.history[\"g_loss\"])","metadata":{"collapsed":false,"ExecuteTime":{"end_time":"2024-12-12T20:20:37.531658Z","start_time":"2024-12-12T20:20:37.463187Z"},"jupyter":{"outputs_hidden":false},"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T21:52:44.681010Z","iopub.execute_input":"2024-12-12T21:52:44.681306Z","iopub.status.idle":"2024-12-12T21:52:44.907081Z","shell.execute_reply.started":"2024-12-12T21:52:44.681275Z","shell.execute_reply":"2024-12-12T21:52:44.906218Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"slope_gan.compile(keras.optimizers.Adam(learning_rate=0.0002), keras.optimizers.Adam(learning_rate=0.0002), keras.losses.BinaryCrossentropy())\nhistory = slope_gan.fit(monet_norm_data, epochs=100)","metadata":{"collapsed":false,"ExecuteTime":{"end_time":"2024-12-12T20:34:12.117258Z","start_time":"2024-12-12T20:22:19.038271Z"},"jupyter":{"outputs_hidden":false},"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T21:52:44.908368Z","iopub.execute_input":"2024-12-12T21:52:44.909159Z","iopub.status.idle":"2024-12-12T21:57:34.243040Z","shell.execute_reply.started":"2024-12-12T21:52:44.909112Z","shell.execute_reply":"2024-12-12T21:57:34.242335Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.plot(history.history[\"d_loss\"])\nplt.plot(history.history[\"g_loss\"])","metadata":{"collapsed":false,"ExecuteTime":{"end_time":"2024-12-12T20:34:12.191733Z","start_time":"2024-12-12T20:34:12.118362Z"},"jupyter":{"outputs_hidden":false},"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T21:57:34.244792Z","iopub.execute_input":"2024-12-12T21:57:34.245071Z","iopub.status.idle":"2024-12-12T21:57:34.494385Z","shell.execute_reply.started":"2024-12-12T21:57:34.245042Z","shell.execute_reply":"2024-12-12T21:57:34.493387Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"leaky_kernel_gan.compile(keras.optimizers.Adam(learning_rate=0.0002), keras.optimizers.Adam(learning_rate=0.0002), keras.losses.BinaryCrossentropy())\nhistory = leaky_kernel_gan.fit(monet_norm_data, epochs=100)","metadata":{"collapsed":false,"ExecuteTime":{"end_time":"2024-12-12T20:41:33.256993Z","start_time":"2024-12-12T20:34:43.267794Z"},"jupyter":{"outputs_hidden":false},"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T21:57:34.495763Z","iopub.execute_input":"2024-12-12T21:57:34.496474Z","iopub.status.idle":"2024-12-12T22:00:14.368521Z","shell.execute_reply.started":"2024-12-12T21:57:34.496426Z","shell.execute_reply":"2024-12-12T22:00:14.367729Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.plot(history.history[\"d_loss\"])\nplt.plot(history.history[\"g_loss\"])","metadata":{"collapsed":false,"ExecuteTime":{"end_time":"2024-12-12T20:41:33.312371Z","start_time":"2024-12-12T20:41:33.259002Z"},"jupyter":{"outputs_hidden":false},"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T22:00:14.369990Z","iopub.execute_input":"2024-12-12T22:00:14.370675Z","iopub.status.idle":"2024-12-12T22:00:14.624070Z","shell.execute_reply.started":"2024-12-12T22:00:14.370631Z","shell.execute_reply":"2024-12-12T22:00:14.623112Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#To visualize how well each GAN predict on a random vector generated to represent vectors from the latent space\nrandom_latent_vector = keras.random.normal(shape=(1, 128), seed=42)\n","metadata":{"collapsed":false,"ExecuteTime":{"end_time":"2024-12-12T20:50:21.264229Z","start_time":"2024-12-12T20:50:21.260435Z"},"jupyter":{"outputs_hidden":false},"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T22:02:37.566592Z","iopub.execute_input":"2024-12-12T22:02:37.567491Z","iopub.status.idle":"2024-12-12T22:02:37.576488Z","shell.execute_reply.started":"2024-12-12T22:02:37.567455Z","shell.execute_reply":"2024-12-12T22:02:37.575681Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"base_imgs = base_gan.generator.predict(random_latent_vector)\nplt.imshow(base_imgs[0])","metadata":{"collapsed":false,"ExecuteTime":{"end_time":"2024-12-12T20:50:22.726123Z","start_time":"2024-12-12T20:50:22.585366Z"},"jupyter":{"outputs_hidden":false},"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T22:02:41.282681Z","iopub.execute_input":"2024-12-12T22:02:41.283522Z","iopub.status.idle":"2024-12-12T22:02:42.376866Z","shell.execute_reply.started":"2024-12-12T22:02:41.283485Z","shell.execute_reply":"2024-12-12T22:02:42.375875Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"g_slope_imgs = g_slope_gan.generator.predict(random_latent_vector)\nplt.imshow(g_slope_imgs[0])","metadata":{"collapsed":false,"ExecuteTime":{"end_time":"2024-12-12T20:52:01.913253Z","start_time":"2024-12-12T20:52:01.741249Z"},"jupyter":{"outputs_hidden":false},"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T22:02:49.188723Z","iopub.execute_input":"2024-12-12T22:02:49.189100Z","iopub.status.idle":"2024-12-12T22:02:49.768922Z","shell.execute_reply.started":"2024-12-12T22:02:49.189067Z","shell.execute_reply":"2024-12-12T22:02:49.768008Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"kernel_imgs = kernel_gan.generator.predict(random_latent_vector)\nplt.imshow(kernel_imgs[0])","metadata":{"collapsed":false,"ExecuteTime":{"end_time":"2024-12-12T20:50:47.417261Z","start_time":"2024-12-12T20:50:47.248331Z"},"jupyter":{"outputs_hidden":false},"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T22:02:53.737021Z","iopub.execute_input":"2024-12-12T22:02:53.737879Z","iopub.status.idle":"2024-12-12T22:02:54.581783Z","shell.execute_reply.started":"2024-12-12T22:02:53.737837Z","shell.execute_reply":"2024-12-12T22:02:54.580847Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"d_slope_img = d_slope_gan.generator.predict(random_latent_vector)\nplt.imshow(d_slope_img[0])","metadata":{"collapsed":false,"ExecuteTime":{"end_time":"2024-12-12T20:53:30.733947Z","start_time":"2024-12-12T20:53:30.566831Z"},"jupyter":{"outputs_hidden":false},"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T22:02:57.020755Z","iopub.execute_input":"2024-12-12T22:02:57.021460Z","iopub.status.idle":"2024-12-12T22:02:57.254116Z","shell.execute_reply.started":"2024-12-12T22:02:57.021425Z","shell.execute_reply":"2024-12-12T22:02:57.253204Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"slope_img = slope_gan.generator.predict(random_latent_vector)\nplt.imshow(slope_img[0])","metadata":{"collapsed":false,"ExecuteTime":{"end_time":"2024-12-12T20:54:40.060988Z","start_time":"2024-12-12T20:54:39.925675Z"},"jupyter":{"outputs_hidden":false},"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T22:02:59.104397Z","iopub.execute_input":"2024-12-12T22:02:59.105116Z","iopub.status.idle":"2024-12-12T22:02:59.340683Z","shell.execute_reply.started":"2024-12-12T22:02:59.105075Z","shell.execute_reply":"2024-12-12T22:02:59.339760Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"leaky_kernel_img = leaky_kernel_gan.generator.predict(random_latent_vector)\nplt.imshow(leaky_kernel_img[0])","metadata":{"collapsed":false,"ExecuteTime":{"end_time":"2024-12-12T20:55:09.017610Z","start_time":"2024-12-12T20:55:08.850029Z"},"jupyter":{"outputs_hidden":false},"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T22:03:01.349499Z","iopub.execute_input":"2024-12-12T22:03:01.349884Z","iopub.status.idle":"2024-12-12T22:03:01.662584Z","shell.execute_reply.started":"2024-12-12T22:03:01.349852Z","shell.execute_reply":"2024-12-12T22:03:01.661773Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Based on the above data the 'slope' GAN performed the best, additionally modifying the discriminator appeared to have little affect\nfinal_GAN = slope_gan\nsubmission_latent_vector = keras.random.normal(shape=(8000, 128), seed=42)\nsubmission_imgs = final_GAN.generator.predict(submission_latent_vector)","metadata":{"collapsed":false,"ExecuteTime":{"end_time":"2024-12-12T21:18:54.960867Z","start_time":"2024-12-12T21:18:09.764764Z"},"jupyter":{"outputs_hidden":false},"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T22:04:09.912957Z","iopub.execute_input":"2024-12-12T22:04:09.913319Z","iopub.status.idle":"2024-12-12T22:04:33.436469Z","shell.execute_reply.started":"2024-12-12T22:04:09.913285Z","shell.execute_reply":"2024-12-12T22:04:33.435286Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission_imgs*=255\nsubmission_imgs.shape\nsubmission_imgs[56]","metadata":{"collapsed":false,"ExecuteTime":{"end_time":"2024-12-12T21:18:54.986063Z","start_time":"2024-12-12T21:18:54.961877Z"},"jupyter":{"outputs_hidden":false},"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T22:04:36.844479Z","iopub.execute_input":"2024-12-12T22:04:36.845244Z","iopub.status.idle":"2024-12-12T22:04:36.888695Z","shell.execute_reply.started":"2024-12-12T22:04:36.845207Z","shell.execute_reply":"2024-12-12T22:04:36.887738Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Resize the image to 256x256 as per the Kaggle submission guidelines\nkaggle_imgs = tf.image.resize(submission_imgs,[256,256])","metadata":{"collapsed":false,"ExecuteTime":{"end_time":"2024-12-12T21:19:52.566706Z","start_time":"2024-12-12T21:19:51.477537Z"},"jupyter":{"outputs_hidden":false},"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T22:04:44.200620Z","iopub.execute_input":"2024-12-12T22:04:44.201312Z","iopub.status.idle":"2024-12-12T22:04:44.809726Z","shell.execute_reply.started":"2024-12-12T22:04:44.201279Z","shell.execute_reply":"2024-12-12T22:04:44.808984Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import zipfile\nfrom io import BytesIO\ni = 0\nwith zipfile.ZipFile(\"/kaggle/working/images.zip\", \"w\") as zip_file:\n    for img in kaggle_imgs:\n        jpg = Image.fromarray(img.numpy().astype(np.uint8))\n        temp = BytesIO()\n        jpg.save(temp, format=\"jpeg\")\n        temp.seek(0)\n        zip_file.writestr(f\"Generated_Image_{i}.jpg\", temp.read())\n        i+=1","metadata":{"collapsed":false,"ExecuteTime":{"end_time":"2024-12-12T21:20:01.249270Z","start_time":"2024-12-12T21:19:54.156397Z"},"jupyter":{"outputs_hidden":false},"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T22:07:17.173477Z","iopub.execute_input":"2024-12-12T22:07:17.173871Z","iopub.status.idle":"2024-12-12T22:07:28.228734Z","shell.execute_reply.started":"2024-12-12T22:07:17.173839Z","shell.execute_reply":"2024-12-12T22:07:28.227726Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"In Conclusion, modifications to the discriminator had little effect on the results. For the generator, there was a performance boost when making the negative slope of the LeakyReLu larger and by making the filter size smaller in the generator. Generally, the GAN with the smallest generator loss was still was making a cloudy image. The next step, would be to look into longer training of the GANs as well as utilizing an encoder to convert the photos into the latent space instead of relying on random vectors.\n\nReferences:\n1. Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks https://arxiv.org/abs/1511.06434.\n2. DCGAN to generate face images https://keras.io/examples/generative/dcgan_overriding_train_step/#train-the-endtoend-model","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false}}}]}